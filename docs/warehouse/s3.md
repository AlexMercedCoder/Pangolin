# AWS S3 Warehouse Configuration

Configure AWS S3 as a warehouse for storing Iceberg table data in Pangolin.

## Overview

AWS S3 is the most common object storage for Iceberg tables, offering:
- High durability (99.999999999%)
- Excellent performance
- Global availability
- Rich ecosystem and tooling
- Cost-effective storage tiers

## Warehouse Configuration

### Option 1: With Static Credentials (Development)

```bash
curl -X POST http://localhost:8080/api/v1/warehouses \
  -H "X-Pangolin-Tenant: my-tenant" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "dev-s3",
    "storage_type": "s3",
    "bucket": "my-dev-datalake",
    "region": "us-east-1",
    "use_sts": false,
    "credentials": {
      "access_key_id": "AKIAIOSFODNN7EXAMPLE",
      "secret_access_key": "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    }
  }'
```

**Use When**:
- Local development
- Testing
- Simple deployments

**Security Note**: Static credentials don't expire. Use STS for production.

### Option 2: With IAM Role / STS (Production - Recommended)

```bash
curl -X POST http://localhost:8080/api/v1/warehouses \
  -H "X-Pangolin-Tenant: my-tenant" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "prod-s3",
    "storage_type": "s3",
    "bucket": "my-prod-datalake",
    "region": "us-east-1",
    "use_sts": true,
    "role_arn": "arn:aws:iam::123456789012:role/PangolinDataAccess",
    "session_duration": 3600
  }'
```

**Use When**:
- Production deployments
- Need temporary credentials
- Want fine-grained permissions

#### IAM Role Setup

1. **Create IAM Role**:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:role/PangolinServerRole"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
```

2. **Attach Policy**:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::my-prod-datalake/*",
        "arn:aws:s3:::my-prod-datalake"
      ]
    }
  ]
}
```

3. **Grant Pangolin Server Permission to Assume Role**:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sts:AssumeRole",
      "Resource": "arn:aws:iam::123456789012:role/PangolinDataAccess"
    }
  ]
}
```

## Client Configuration

### Scenario 1: Catalog WITH Warehouse (Recommended)

Pangolin automatically vends credentials to clients. No client-side S3 configuration needed!

#### PyIceberg

```python
from pyiceberg.catalog import load_catalog

# Pangolin vends credentials automatically via X-Iceberg-Access-Delegation header
catalog = load_catalog(
    "pangolin",
    **{
        "uri": "http://localhost:8080/api/v1/catalogs/analytics",
        "warehouse": "s3://my-prod-datalake/analytics/"
    }
)

# Create namespace
catalog.create_namespace("db")

# Create table - Pangolin handles S3 access
from pyiceberg.schema import Schema
from pyiceberg.types import NestedField, StringType, IntegerType

schema = Schema(
    NestedField(1, "id", IntegerType(), required=True),
    NestedField(2, "name", StringType(), required=True)
)

table = catalog.create_table(
    "db.users",
    schema=schema
)

# Write data - credentials vended automatically
import pyarrow as pa

data = pa.table({
    "id": [1, 2, 3],
    "name": ["Alice", "Bob", "Charlie"]
})

table.append(data)

# Read data
df = table.scan().to_pandas()
print(df)
```

#### PySpark

```python
from pyspark.sql import SparkSession

# Pangolin vends credentials automatically
spark = SparkSession.builder \
    .appName("Pangolin Iceberg") \
    .config("spark.sql.catalog.pangolin", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.pangolin.catalog-impl", "org.apache.iceberg.rest.RESTCatalog") \
    .config("spark.sql.catalog.pangolin.uri", "http://localhost:8080/api/v1/catalogs/analytics") \
    .config("spark.sql.catalog.pangolin.warehouse", "s3://my-prod-datalake/analytics/") \
    .getOrCreate()

# Create table
spark.sql("""
    CREATE TABLE pangolin.db.users (
        id INT,
        name STRING
    ) USING iceberg
""")

# Write data
data = [(1, "Alice"), (2, "Bob"), (3, "Charlie")]
df = spark.createDataFrame(data, ["id", "name"])
df.writeTo("pangolin.db.users").append()

# Read data
spark.table("pangolin.db.users").show()
```

### Scenario 2: Catalog WITHOUT Warehouse

Clients must configure S3 access themselves.

#### PyIceberg

```python
from pyiceberg.catalog import load_catalog
import os

# Set AWS credentials
os.environ["AWS_ACCESS_KEY_ID"] = "AKIAIOSFODNN7EXAMPLE"
os.environ["AWS_SECRET_ACCESS_KEY"] = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
os.environ["AWS_REGION"] = "us-east-1"

catalog = load_catalog(
    "pangolin",
    **{
        "uri": "http://localhost:8080/api/v1/catalogs/analytics",
        "warehouse": "s3://my-prod-datalake/analytics/",
        # Client-side S3 configuration
        "s3.region": "us-east-1",
        "s3.access-key-id": "AKIAIOSFODNN7EXAMPLE",
        "s3.secret-access-key": "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
    }
)

# Use catalog normally
table = catalog.load_table("db.users")
df = table.scan().to_pandas()
```

#### PySpark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Pangolin Iceberg") \
    .config("spark.sql.catalog.pangolin", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.pangolin.catalog-impl", "org.apache.iceberg.rest.RESTCatalog") \
    .config("spark.sql.catalog.pangolin.uri", "http://localhost:8080/api/v1/catalogs/analytics") \
    .config("spark.sql.catalog.pangolin.warehouse", "s3://my-prod-datalake/analytics/") \
    .config("spark.hadoop.fs.s3a.access.key", "AKIAIOSFODNN7EXAMPLE") \
    .config("spark.hadoop.fs.s3a.secret.key", "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY") \
    .config("spark.hadoop.fs.s3a.endpoint.region", "us-east-1") \
    .getOrCreate()

# Use Spark normally
spark.table("pangolin.db.users").show()
```

## S3 Best Practices

### Performance
1. **Use S3 Transfer Acceleration** for cross-region access
2. **Enable S3 Intelligent-Tiering** for cost optimization
3. **Use Requester Pays** for shared datasets
4. **Collocate compute and storage** in same region

### Security
1. **Enable S3 Bucket Encryption** (SSE-S3 or SSE-KMS)
2. **Use VPC Endpoints** to avoid internet traffic
3. **Enable S3 Access Logging** for audit trails
4. **Use Bucket Policies** to restrict access
5. **Enable Versioning** for data protection

### Cost Optimization
1. **Use Lifecycle Policies** to transition old data to Glacier
2. **Enable S3 Intelligent-Tiering** for automatic cost optimization
3. **Use S3 Storage Lens** to analyze usage
4. **Compress Parquet files** with Snappy or Zstd

### Organization
1. **Bucket Structure**: `s3://bucket/{catalog}/{namespace}/{table}/`
2. **Naming Convention**: Use descriptive, hierarchical names
3. **Separate Buckets**: Use different buckets for dev/staging/prod
4. **Cross-Region Replication**: For disaster recovery

## Troubleshooting

### Access Denied

```
Error: Access Denied (Service: Amazon S3; Status Code: 403)
```

**Solutions**:
1. Check IAM role permissions
2. Verify bucket policy
3. Check STS assume role permissions
4. Ensure bucket exists and is in correct region

### Slow Performance

**Solutions**:
1. Check region - ensure compute and S3 are colocated
2. Enable S3 Transfer Acceleration
3. Use larger instance types
4. Check network bandwidth
5. Use S3 Select for filtering

### Credential Vending Not Working

```
Error: No credentials provided
```

**Solutions**:
1. Ensure catalog has warehouse attached
2. Check warehouse `use_sts` setting
3. Verify IAM role ARN is correct
4. Check Pangolin server has permission to assume role

## Monitoring

### CloudWatch Metrics

Monitor these S3 metrics:
- `BucketSizeBytes` - Total bucket size
- `NumberOfObjects` - Object count
- `AllRequests` - Request rate
- `4xxErrors` - Client errors
- `5xxErrors` - Server errors

### CloudTrail

Enable CloudTrail to audit S3 access:
```bash
aws cloudtrail create-trail \
  --name pangolin-s3-audit \
  --s3-bucket-name my-audit-bucket
```

## Additional Resources

- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)
- [S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/best-practices.html)
- [S3 Performance Optimization](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [Iceberg S3 Configuration](https://iceberg.apache.org/docs/latest/aws/)

## Next Steps

- [Azure Blob Warehouse](azure.md)
- [GCS Warehouse](gcs.md)
- [Warehouse Concept](README.md)
- [Backend Storage](../backend_storage/README.md)
